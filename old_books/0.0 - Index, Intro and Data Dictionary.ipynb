{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96404ab0",
   "metadata": {},
   "source": [
    "# DunnHumby: The Complete Journey\n",
    "--- \n",
    "    Nathaniel Poland\n",
    "    Feb 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88b119d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to my Capstone Project! \n",
    "\n",
    "In this project we will explore **The Complete Journey**; a dataset available on Kaggle and generated by the Dunnhumby company with the assistance of Raj Venkatesan. This dataset was suggested to me by Myles Harrison, my instructor at the BrainStation Data Science and Analytics bootcamp which I completed over the Summer 2021.\n",
    "\n",
    "I'm deeply grateful for the recommendation, and wrestling with this data I have learned and practiced so many things that have served me well as I continue on my own 'complete journey' into data analytics and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d343d71",
   "metadata": {},
   "source": [
    "# The Problem Space\n",
    "\n",
    "This dataset is purportedly from a grocery chain firm. The data comprises **8 .csv files**, and resembles a **relational database**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbcfe7c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# The Data\n",
    "\n",
    "- The Complete Journey is available through the Kaggle API at [https://www.kaggle.com/frtgnn/dunnhumby-the-complete-journey](https://www.kaggle.com/frtgnn/dunnhumby-the-complete-journey)\n",
    "- There is a data dictionary produced by the company available online (I searched Google):\n",
    "[https://pdfcoffee.com/dunnhumby-the-complete-journey-user-guide-pdf-free.html](https://pdfcoffee.com/dunnhumby-the-complete-journey-user-guide-pdf-free.html)\n",
    "- It was originally generated and offered by DunnHumby, for use in classrooms.\n",
    "\n",
    "The data comes in the form of **8 .csv files:**\n",
    "\n",
    "Data Dictionary:\n",
    "---\n",
    "## `Info about Sales and Products:`\n",
    "\n",
    "### **transaction_data.csv**\n",
    "- 2.6m rows, 12 columns of item-level transactions\n",
    "- Represents 711 days of 'basket' purchases by 2500 households.\n",
    "- Includes sales information including loyalty-program discounts, as well as coupon discounts and coupon match discounts.\n",
    "    - 'DAY', \n",
    "    - 'PRODUCT_ID', \n",
    "    - 'BASKET_ID', \n",
    "    - 'household_key', \n",
    "    - 'SALES_VALUE',...\n",
    "\n",
    "### **product.csv**\n",
    "- 92k rows, 4 columns of unique product descriptions;\n",
    "- Reference list to further describe products in the transactions table.\n",
    "    - 'PRODUCT_ID', \n",
    "    - 'DEPARTMENT', \n",
    "    - 'COMMODITY_DESC', \n",
    "    - 'SUB_COMMODITY_DESC'\n",
    "\n",
    "## `Info about customers/households:`\n",
    "\n",
    "### **hh_demographic.csv**\n",
    "- 801 rows of demographic information for unique households (customers)\n",
    "- Customer survey information;\n",
    "- Only 801/2500 households have this information\n",
    "    - 'AGE_DESC', \n",
    "    - 'MARITAL_STATUS_CODE', \n",
    "    - 'INCOME_DESC', \n",
    "    - 'HOUSEHOLD_SIZE_DESC',...\n",
    "\n",
    "## `Info about direct marketing campaigns:`\n",
    "\n",
    "### **causal_data.csv**\n",
    "- 38m rows of item-specific feature information;\n",
    "- store-specific flyer, and display locations, respectively\n",
    "    - 'PRODUCT_ID', \n",
    "    - 'WEEK_NO', \n",
    "    - 'mailer', \n",
    "    - 'display', \n",
    "    - 'STORE_ID'\n",
    "\n",
    "### **campaign_desc.csv**\n",
    "- 30 rows;\n",
    "- thirty individual advertising campaigns\n",
    "    - 'DESCRIPTION', \n",
    "    - 'CAMPAIGN', \n",
    "    - 'START_DAY', \n",
    "    - 'END_DAY'\n",
    "\n",
    "### **campaign_table.csv**\n",
    "- 7k rows;\n",
    "- households targeted by each campaign\n",
    "    - 'DESCRIPTION', \n",
    "    - 'household_key', \n",
    "    - 'CAMPAIGN'\n",
    "\n",
    "\n",
    "## `Info about manufacturer coupons:`\n",
    "\n",
    "### **coupon_redempt.csv**\n",
    "- 2318 rows; \n",
    "- Seemingly not a ton of engagement with manufacturer coupons(?)\n",
    "    - 'household_key', \n",
    "    - 'DAY', \n",
    "    - 'COUPON_UPC', \n",
    "    - 'CAMPAIGN'\n",
    "\n",
    "### **coupon.csv**\n",
    "- 124k rows; \n",
    "- reference list for coupon_redempt by campaign timeframe -- (across all stores?)\n",
    "    - 'COUPON_UPC', \n",
    "    - 'PRODUCT_ID', \n",
    "    - 'CAMPAIGN'\n",
    "    \n",
    "None of the tables have any null values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07687614",
   "metadata": {},
   "source": [
    "\n",
    "The data consists of ~2 years of in-store transaction data for 2500 frequently-purchasing households at a grocery chain; as well as coupon redemption information from those households. We'll need to verify the integrity of this data (in so doing, set up repeatable processes which could be performed on new data in this format which might become available).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a131b9fe",
   "metadata": {},
   "source": [
    "\n",
    "...after eda...\n",
    "There are several factors to take into account about this use case and the available data:\n",
    "    - although there is a lot of data available, it is fairly unclean, and it is unlikely we'll get more data.\n",
    "    - we don't have access to the total sales numbers of all the stores, nor their product-level sales, nor their geographic locations..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79879c0b",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "The analysis to follow will centre around these purchases and the households which made them; with the purpose of examining:\n",
    "\n",
    "    - what types of customers are these frequently-purchasing households?\n",
    "    - what can we say about their purchase patterns?\n",
    "    \n",
    "    ...later...\n",
    "    - recommender system for individual households...\n",
    "       - for individual items?\n",
    "   \n",
    "\n",
    "The sales data is mostly contained in just 2 of the 8 tables; `transaction_data.csv` and `products.csv`. We will focus most of our attention on distinguishing customer groups through market segmentation; and defining the purchase behaviour for those groups using clean and labeled transaction information.\n",
    "\n",
    "The remaining 6 tables contain information regarding advertising campaigns which were sent out to different households in the data, as well as demographic information for about a third of the households. \n",
    "\n",
    "So, given this dataset (and being naturally skeptical of its integrity), we are tasked with **exploring it and deriving what insights we can**. I would like to do the following, to form a basic report:\n",
    "\n",
    "- EDA on transactions, products, demographics\n",
    "- Customer Segmentation/Labelling\n",
    "- Implementing a Recommender System for distinct customer groups. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f20ade",
   "metadata": {},
   "source": [
    "## Data Scientist? What's that?\n",
    "\n",
    "What does it mean to be a data scientist? It lies at the intersection of programming, statistics, and communication skills. Tasked with identifying meaningful patterns in data, the Data Scientist has many responsibilities... \n",
    "\n",
    "**Business Outcomes**\n",
    "\n",
    "Specifically focusing around business outcomes, the data scientist lies at an important juncture between data engineers, business leaders, and business analysts. Serving the needs of coworkers or clients, respectively, the data scientist's responsibility is to surface relevant data insights to power their work or business forward, as well as providing context for how to develop data infrastructure and models to better serve those needs in the future.\n",
    "\n",
    "**Data Analysis**\n",
    "\n",
    "With data in hand, we look to create some sort of presentation of derived insights -- be it to a board room or coworkers. Only from this place of basic confidence about what we're working with can we move forwards into the realm of statistics and machine learning, and expect consistent and causal benefits. \n",
    "\n",
    "As a new data scientist, my impression is that quite a large part of the job has nothing to do with statistics or even (sometimes) machine learning (which is a bit of a bummer!). So much information can be derived simply from analytics and common sense, and I love the way data can paint a picture of the world around us, or address the needs of a client in a specific use case. \n",
    "\n",
    "**Many Hats**\n",
    "\n",
    "The tasks a 'data scientist' might be asked to perform will vary widely across positions. Statistical modelling or ML/AI techniques can be employed on clean and pertinent data to reveal correlations or other patterns in the underlying information. \n",
    "\n",
    "**Modelling and Data Infrastructure**\n",
    "\n",
    "Good data infrastructure and clear business outcomes (target metrics) prepare you for success; strong data preparation  are what lead to great modelling results -- and all of that adds up to creating positive value for the client. Relationships should not be taken as causality without sufficient evidence, but we do hope to prove something that we weren't sure about before, in the case of data analysis and statistics; or to predict something to come, with an acceptable level of error, in the case of machine learning models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d598aa36",
   "metadata": {},
   "source": [
    "# Table Structure\n",
    "## Table 1: Household Profile\n",
    "Optimized for maximum depth of customer information.\n",
    "\n",
    "## Table 2: Apriori/Association Rules for Customer Segments\n",
    "Populate recommendations by recent purchases; use a few tables for similar customers\n",
    "\n",
    "This repo hopes to be a full data project, barring the data acquisition phase. \n",
    "\n",
    "- data exploration, cleaning\n",
    "- setting a scope for the project\n",
    "- identifying and justifying the use of ML for business outcomes\n",
    "        \n",
    "Developing infrastructure and tools:\n",
    "\n",
    "- creating a Python package which performs ETL and other tasks; accounts for issues in the data source; cleaning, transformations, ad-hoc dashboard requests\n",
    "\n",
    "Implementing a data product with business-facing value on a remote server/environment\n",
    "\n",
    "---\n",
    "I am not an expert in these fields, and I'd love to learn from someone who knows more. I've tried to learn the technical skills which might make me contributing junior member of a team who are taking on similar tasks. \n",
    "\n",
    "NEXT STEPS:\n",
    "\n",
    "- SQL-based ETL\n",
    "- bash-based environment and data deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed70f7d",
   "metadata": {},
   "source": [
    "## Business Questions\n",
    "\n",
    "`Customers`:\n",
    "- What can be said about the customers (households) in the data? \n",
    "    - How can they be said to be similar; how can they be said to be different? \n",
    "    - Which types of customers are purchasing more over time? Less over time?\n",
    "    - Which products are customers purchasing more or less of?\n",
    "    \n",
    "`Sales`:\n",
    "- What is the nature of the sales data?\n",
    "    - What products or product categories generate the most sales revenue?\n",
    "    - Is there a difference in the products purchased by customers of different categories?\n",
    "\n",
    "`Advertising`:\n",
    "- What was the effect of the 30 distinct advertising campaigns on purchase behavior?\n",
    "    - Which households were most affected by direct marketing?\n",
    "    - Is there a statistically significant impact on purchase behaviour, for any of the customer groups, based on direct marketing?\n",
    "    \n",
    "    \n",
    "**What sort of 'data product' can I create, which offers client-facing value, machine learning know-how, and showcases my own learning process and thinking along the way?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d985cc",
   "metadata": {},
   "source": [
    "## Initial Impressions of the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eba6c0",
   "metadata": {},
   "source": [
    "`hh_demographic`, `product`: look straightforward, but we only have 801 households with demographic information. We'll have to compare the list of products to the list of transactions and promotions we have on record. Both tables are tied by `PRODUCT_ID` to the transactions table.\n",
    "\n",
    "`causal_data`, `transactions`: Tons of rows, to the point of being unfeasible to load into local memory. causal_data.csv, per the data dictionary, is the placement of individual products over in certain store areas; as well as the same product's location in the mailer flyers on a week-by-week basis. Transactions is all of the sales data we have.\n",
    "\n",
    "`campaign_desc`, `campaign_table`: descriptions of campaigns and when each was active.\n",
    "\n",
    "`coupon`, `coupon_redempt`: descriptions of coupons and their redemptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2733b35c",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#import modules \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "#import modules \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.style.use('seaborn')\n",
    "import seaborn as sns\n",
    "\n",
    "import datetime\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d1ebf4",
   "metadata": {},
   "source": [
    "The Dataset has 8 tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61bdc525",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of .csv files: \n",
      "campaign_desc\n",
      "campaign_table\n",
      "causal_data\n",
      "coupon\n",
      "coupon_redempt\n",
      "hh_demographic\n",
      "product\n",
      "transaction_data\n"
     ]
    }
   ],
   "source": [
    "# printing filenames\n",
    "\n",
    "# STORE DATA IN MEMORY?\n",
    "# i'd like to learn about weakref or similar;\n",
    "# archive = dict()\n",
    "\n",
    "files = glob.glob('data/*.csv')\n",
    "print('List of .csv files: ')\n",
    "for file in files:\n",
    "    filename = file[5:-4]\n",
    "    print(filename)\n",
    "# Verbose Readout for Load\n",
    "#     print(\"*\"*40)\n",
    "#     df = pd.read_csv(file)\n",
    "#     print(f'Shape : {df.shape}')\n",
    "#     print(f\"Nulls: {df.isna().sum().sum()}\")\n",
    "\n",
    "# store all tables in active memory? dangerous.\n",
    "#       archive[filename] = df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3536d46",
   "metadata": {},
   "source": [
    "#### Table Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3d2466",
   "metadata": {},
   "source": [
    "A visual description of their inter-relationships, column names, and number of rows is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08f49f5c",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>shape</th>\n",
       "      <th>nulls</th>\n",
       "      <th>num_cols</th>\n",
       "      <th>columns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>campaign_desc</td>\n",
       "      <td>(30, 4)</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[DESCRIPTION, CAMPAIGN, START_DAY, END_DAY]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>campaign_table</td>\n",
       "      <td>(7208, 3)</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[DESCRIPTION, household_key, CAMPAIGN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>causal_data</td>\n",
       "      <td>(36786524, 5)</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>[PRODUCT_ID, STORE_ID, WEEK_NO, display, mailer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>coupon</td>\n",
       "      <td>(124548, 3)</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[COUPON_UPC, PRODUCT_ID, CAMPAIGN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>coupon_redempt</td>\n",
       "      <td>(2318, 4)</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[household_key, DAY, COUPON_UPC, CAMPAIGN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hh_demographic</td>\n",
       "      <td>(801, 8)</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>[AGE_DESC, MARITAL_STATUS_CODE, INCOME_DESC, H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>product</td>\n",
       "      <td>(92353, 7)</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>[PRODUCT_ID, MANUFACTURER, DEPARTMENT, BRAND, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>transaction_data</td>\n",
       "      <td>(2595732, 12)</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>[household_key, BASKET_ID, DAY, PRODUCT_ID, QU...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name          shape  nulls  num_cols  \\\n",
       "0     campaign_desc        (30, 4)      0         4   \n",
       "1    campaign_table      (7208, 3)      0         3   \n",
       "2       causal_data  (36786524, 5)      0         5   \n",
       "3            coupon    (124548, 3)      0         3   \n",
       "4    coupon_redempt      (2318, 4)      0         4   \n",
       "5    hh_demographic       (801, 8)      0         8   \n",
       "6           product     (92353, 7)      0         7   \n",
       "7  transaction_data  (2595732, 12)      0        12   \n",
       "\n",
       "                                             columns  \n",
       "0        [DESCRIPTION, CAMPAIGN, START_DAY, END_DAY]  \n",
       "1             [DESCRIPTION, household_key, CAMPAIGN]  \n",
       "2   [PRODUCT_ID, STORE_ID, WEEK_NO, display, mailer]  \n",
       "3                 [COUPON_UPC, PRODUCT_ID, CAMPAIGN]  \n",
       "4         [household_key, DAY, COUPON_UPC, CAMPAIGN]  \n",
       "5  [AGE_DESC, MARITAL_STATUS_CODE, INCOME_DESC, H...  \n",
       "6  [PRODUCT_ID, MANUFACTURER, DEPARTMENT, BRAND, ...  \n",
       "7  [household_key, BASKET_ID, DAY, PRODUCT_ID, QU...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_dfs():\n",
    "    data_files = glob.glob('data/*.csv')\n",
    "    num_files = len(data_files)\n",
    "    output = pd.DataFrame()\n",
    "    \n",
    "\n",
    "    for idx, file in enumerate(data_files):\n",
    "        df = pd.read_csv(file)\n",
    "        name = file.split(\"\\\\\")[1][:-4]\n",
    "        shape = df.shape\n",
    "        nulls = df.isna().sum().sum()\n",
    "        columns = list(df.columns)\n",
    "        num_cols = len(df.columns)\n",
    "        output = output.append(pd.Series([name, shape, nulls, num_cols, columns]), ignore_index=True)\n",
    "    output.columns =  ['name', 'shape', 'nulls', 'num_cols', 'columns']\n",
    "    output['num_cols'] = output['num_cols'].astype(int)\n",
    "    output['nulls'] = output['nulls'].astype(int)\n",
    "\n",
    "    return output\n",
    "check_dfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de444d72",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def print_histograms():\n",
    "    data_files = glob.glob('data/*.csv')\n",
    "    num_files = len(data_files)\n",
    "    output = pd.DataFrame()\n",
    "    \n",
    "    for idx, file in enumerate(data_files):\n",
    "        df = pd.read_csv(file)\n",
    "        name = file.split(\"\\\\\")[-1][:-4]\n",
    "        shape = df.shape\n",
    "        nulls = df.isna().sum().sum()\n",
    "        columns = list(df.columns)\n",
    "        num_cols = len(df.columns)\n",
    "        rows = 1\n",
    "        \n",
    "        if num_cols > 4:\n",
    "            rows = num_cols//4 + 1\n",
    "            num_cols = num_cols //4 + 1\n",
    "        \n",
    "        # plotting\n",
    "        plt.subplots(rows, num_cols)\n",
    "        plt.suptitle(f'{name.upper()}.csv; {shape[0]} rows and {shape[1]} columns')\n",
    "        for idx, col in enumerate(df.columns):\n",
    "            plt.subplot(rows, num_cols,  idx+1)\n",
    "            plt.title(f'{col.upper()}')\n",
    "            plt.hist(df[col], bins=15)\n",
    "            plt.xticks(rotation='45')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "print_histograms() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183ecdfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
