{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08bb9278",
   "metadata": {},
   "source": [
    "# Dunnhumby: The Complete Journey "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d63f6ab",
   "metadata": {},
   "source": [
    "I'm not a business major, and I know I won't be doing any business-side interactions any time soon; but I have heard (and would tend to agree) that good data science interfaces directly to the goal at hand -- the business outcomes; be they for business or research. This means **looking at evaluation metrics, key performance indicators, and return on investment for the employer.**  \n",
    "\n",
    "In writing this project, Ideas developed about **what a data product needs to be doing**; how to approach data cleaning, analysis and modelling in a modular and structured way; and I've been on a path of discovery to learn how to implement my analysis and modelling techniques into a usable product which provides business value -- to my team, or to the client.\n",
    "\n",
    "I'm happy to say I was able to deploy an app related to this project on a remote server using `Heroku` and `streamlit`. As far as I'm concerned, the value of learning how to **make my own Python package** far surpasses any value generated from this data set! Just don't tell the boss.\n",
    "\n",
    "Below, I go over some key questions I have for this data, then my interpretation of what ML is about; generally and in the scope of this project; and finally what I've done with the data in The Complete Journey."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faef3b3",
   "metadata": {},
   "source": [
    "# ML Comprehension and Project Scope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d826fe17",
   "metadata": {},
   "source": [
    "Before getting into the EDA, I wanted to touch base about the scope of this project; as well as where I stand in terms of machine learning comprehension.\n",
    "\n",
    "## Scope:\n",
    "\n",
    "- use the transactions and demographics information to label those households for which we have demographic information with relevant or distinguishing **customer labels**;\n",
    "- to then **train a model** to predict those labels with some degree of accuracy for the remaining households in the data using supervised/unsupervised learning;\n",
    "- and finally, to use those customer labels to **make recommendations based on the behaviours of \"similar\" households**.\n",
    "\n",
    "These projected `customer labels` could be derived in many ways. Let's talk a little bit about ML generally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54afaa5",
   "metadata": {},
   "source": [
    "## ML Comprehension: \n",
    "\n",
    "Regression; Decision Tree, and Classification: Supervised and Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a439476",
   "metadata": {},
   "source": [
    "### Data Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9549a7",
   "metadata": {},
   "source": [
    "Many predictive ML models rely on the idea that the `features` (attributes of our data point) which we choose to include in our training set are `relevant`, and contain accurate records of the underlying causal (or correlative) mechanisms which produce (or describe) our `target` feature. These features could be continuous or categorical features.\n",
    "    \n",
    "**This basic assumption of data relevance is a crash course in model (statistical?) bias -- in the end, we are the ones who pick what information goes in.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5bd2aa",
   "metadata": {},
   "source": [
    "### Variance and Scale\n",
    "Many or all predictive ML models rely on the idea that **the `features` `X1-Xn` of our feature space display a range of `variance`** with relation to the `dependent` variable `y`. \n",
    "    \n",
    "That is to say, in order for us to make predictions, then **our features must vary, and along the axes of our feature space, our target must also vary** either categorically or continuously; and with some sort of recognizable pattern or tendency. \n",
    "    \n",
    "    \n",
    "### The Target\n",
    "The `target` could be a continuous feature -- such as height or weight (numeric variables) -- or a discrete feature, such as colour or type (categorical variables). Using some process of label encoding, we can binarize category labels using 'dummy columns', such that the model can interpret their presence or absence as correlative with other features in a given group of data points. This opens up the option of logistic regression for classification on categorical variables, or derivations thereof.\n",
    "\n",
    "### The Features\n",
    "Our features can be described as distributions; we typically normalize these spectrums of variance between [-1,1] or [0,1] such as to eliminate the impact of unit scale -- we don't want to compare centimeters to miles. We might also transform the data in other ways as well, such that what remains is the **discernible spectrum of variance in that feature**. So long as we are evaluating our models based on some form of euclidean distance, this results in similar data points being grouped together.\n",
    "\n",
    "### Transformations and Transfer Learning\n",
    "There are ways to convert data of almost any form into recognizable input for some of these models. Some problems, like NLP or image recognition, use the concept of 'transfer learning', where a model has been trained to some level of basic competency at predicting the similarity of data points within a given context; for example `word2vec` or `ResNet`. Similar features in these trained models are 'grouped' with one another; in the case of NLP this means that the `token`'s vectors have a high degree of cosine similarity. The trained models have **grouped similar words or images with one another ahead of time**, and can be loaded as such and then further trained for a specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eedb647",
   "metadata": {},
   "source": [
    "# ML Goals for The Complete Journey\n",
    "\n",
    "## Business Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd41533",
   "metadata": {},
   "source": [
    "\n",
    "## Scope:\n",
    "\n",
    "- use the transactions and demographics information to label those households for which we have demographic information with relevant or distinguishing **customer labels**;\n",
    "- to then **train a model** to predict those labels with some degree of accuracy for the remaining households in the data using supervised/unsupervised learning;\n",
    "- and finally, to use those customer labels to **make recommendations based on the behaviours of \"similar\" households**.\n",
    "\n",
    "These projected `customer labels` could be derived in many ways. Let's talk a little bit about ML generally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f4b929",
   "metadata": {},
   "source": [
    "For this data I would like to do four things:\n",
    "\n",
    "- Time Series Sales Analysis\n",
    "- Customer Segmentation by formula, and through Supervised and Unsupervised Learning\n",
    "- Association Rules through FPGrowth for Customer Recommendations\n",
    "- OLS for Advertising Campaign Sales\n",
    "\n",
    "In order to do this modelling, we must ensure the integrity of our data, as well as descriptive labels; and we must frame the features of each data point in such a way as to not confuse the model. In order to predict what we want to predict, be it sales forecasts, or customer demographics; we must `train` an ML model to recognize patterns in the our feature space which can predict the `outcome` of the target. \n",
    "\n",
    "- In the context of time-series, this means creating `stationarity` and removing `seasonality` in the data. \n",
    "- For customer segmentation this might involve removing outliers, or transforming columns, and `visualizing purchase behaviour`. \n",
    "- Association rules algorithms rely on descriptive and consolidated `product labels` in order to be effective. \n",
    "- In order to perform other statistical tests, we often make the assumption that the underlying data comes from a normal distribution, or that the events described are independent of one another. \n",
    "\n",
    "**Our models are going to miss sometimes. We have to understand what the fallout of either a false negative or a false positive -- this is the idea of precision and recall.**\n",
    "\n",
    "# Data Preparation; Lakes or Streams?\n",
    "\n",
    "When dealing with the large amount of data and many tables in this project, I often found myself re-transforming my data in different workbooks -- highly inefficient, and frustrating. I decided to set up an ETL pipeline (although I didn't call it that) as well as leverage GitHub in order to make my code more practical.\n",
    "\n",
    "It was a hell of an experience, but I'm so glad I did. I learned how to write my own Python package, and am excited to implement similar concepts in SQL- or bash- based ETL processes in the future. Doing everything in Python was sort of cheating; but it really helped in terms of making the exploration, visualization, and cleaning more simple. \n",
    "\n",
    "# Data Product\n",
    "\n",
    "This process was a simplified version of what I imagine might exist in industry:\n",
    "- a front-end API which parses data from customer interactions and stores it\n",
    "- automated and scheduled processing of previous data to form customer labels and run computation-heavy tasks\n",
    "- a back-end API which allows access to customer profiles; the creation of dashboards; and stored recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcc9613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4c9ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
